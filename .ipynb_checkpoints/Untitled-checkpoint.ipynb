{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example of state, action, reward, and next state\n",
      "[[0.59  0.531 0.478 0.43 ]\n",
      " [0.656 0.    0.    0.   ]\n",
      " [0.729 0.81  0.9   0.   ]\n",
      " [0.    0.9   1.    0.   ]]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "### MDP Value Iteration and Policy Iteratoin\n",
    "# You might not need to use all parameters\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, max_iteration=1000, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P: dictionary\n",
    "        It is from gym.core.Environment\n",
    "        P[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "    nS: int\n",
    "        number of states\n",
    "    nA: int\n",
    "        number of actions\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "    policy: np.array\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    max_iteration: int\n",
    "        The maximum number of iterations to run before stopping. Feel free to change it.\n",
    "    tol: float\n",
    "        Determines when value function has converged.\n",
    "    Returns\n",
    "    -------\n",
    "    value function: np.ndarray\n",
    "        The value function from the given policy.\n",
    "    \"\"\"\n",
    "    ############################\n",
    "    # initialize value function\n",
    "    V = np.zeros(nS)\n",
    "    V_prev = np.copy(V)\n",
    "    iter_counter = 0\n",
    "    tol_curr = np.inf\n",
    "    # loop over Bellman updates:\n",
    "    while iter_counter <= max_iteration and tol_curr >= tol:\n",
    "        for s in range(nS):\n",
    "            p_sp_r_t_lst = P[s][policy[s]]\n",
    "            exp_r_v = 0\n",
    "            for p_sp_r_t in p_sp_r_t_lst:\n",
    "                p, sp, r, t = p_sp_r_t\n",
    "                exp_r_v += p * (r + gamma * V_prev[sp])\n",
    "            V[s] = exp_r_v\n",
    "        tol_curr = np.abs(V - V_prev).max()\n",
    "        V_prev = np.copy(V)\n",
    "        iter_counter += 1\n",
    "    ############################\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"Given the value function from policy improve the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P: dictionary\n",
    "        It is from gym.core.Environment\n",
    "        P[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "    nS: int\n",
    "        number of states\n",
    "    nA: int\n",
    "        number of actions\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new policy: np.ndarray\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "    ############################\n",
    "    V = value_from_policy\n",
    "    q_np = np.zeros((nS, nA), dtype='float')\n",
    "    policy_next = np.zeros(nS, dtype='int')\n",
    "    \n",
    "    for s in range(nS):\n",
    "        # argmax Q(s, a)\n",
    "        # Q(s, a) = E[R(s, a) + gamma * V(s')]\n",
    "        for a in P[s].keys():\n",
    "            p_sp_r_t_lst = P[s][a]\n",
    "            exp_r_v = 0.\n",
    "            for p_sp_r_t in p_sp_r_t_lst:\n",
    "                p, sp, r, t = p_sp_r_t\n",
    "                exp_r_v += p * (r + gamma * V[sp])\n",
    "            q_np[s, a] = exp_r_v\n",
    "    policy_next = q_np.argmax(axis=1)\n",
    "    ############################\n",
    "    return policy_next\n",
    "\n",
    "\n",
    "def policy_iteration(P, nS, nA, gamma=0.9, max_iteration=20, tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should use the policy_evaluation and policy_improvement methods to\n",
    "    implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P: dictionary\n",
    "        It is from gym.core.Environment\n",
    "        P[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "    nS: int\n",
    "        number of states\n",
    "    nA: int\n",
    "        number of actions\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "    max_iteration: int\n",
    "        The maximum number of iterations to run before stopping. Feel free to change it.\n",
    "    tol: float\n",
    "        Determines when value function has converged.\n",
    "    Returns:\n",
    "    ----------\n",
    "    value function: np.ndarray\n",
    "    policy: np.ndarray\n",
    "    \"\"\"\n",
    "    V = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    ############################\n",
    "    iter_counter = 0\n",
    "    policy_change = np.inf\n",
    "    while iter_counter == 0 or policy_change > 0:\n",
    "        value_from_policy = policy_evaluation(P, nS, nA, policy, gamma=0.9, max_iteration=max_iteration, tol=tol)\n",
    "        policy_next = policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9)\n",
    "        policy_change = np.abs((policy_next - policy).max())\n",
    "        iter_counter += 1\n",
    "        del policy\n",
    "        policy = np.copy(policy_next)\n",
    "    V = policy_evaluation(P, nS, nA, policy_new, gamma=0.9, max_iteration=max_iteration, tol=tol)\n",
    "    ############################\n",
    "    return V, policy_next\n",
    "\n",
    "def value_iteration(P, nS, nA, gamma=0.9, max_iteration=20, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    P: dictionary\n",
    "        It is from gym.core.Environment\n",
    "        P[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "    nS: int\n",
    "        number of states\n",
    "    nA: int\n",
    "        number of actions\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "    max_iteration: int\n",
    "        The maximum number of iterations to run before stopping. Feel free to change it.\n",
    "    tol: float\n",
    "        Determines when value function has converged.\n",
    "    Returns:\n",
    "    ----------\n",
    "    value function: np.ndarray\n",
    "    policy: np.ndarray\n",
    "    \"\"\"\n",
    "    V = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    ############################\n",
    "    V_next = np.zeros(nS, dtype=int)\n",
    "    q_np = np.zeros((nS, nA), dtype='float')\n",
    "    iter_counter = 0\n",
    "    tol_curr = np.inf\n",
    "    while iter_counter <= max_iteration and tol_curr >= tol:\n",
    "        for s in range(nS):\n",
    "            for a in range(nA):                \n",
    "                p_sp_r_t_lst = P[s][a]\n",
    "                exp_r_v = 0.\n",
    "                for p_sp_r_t in p_sp_r_t_lst:\n",
    "                    p, sp, r, t = p_sp_r_t\n",
    "                    exp_r_v += p * (r + gamma * V[sp])\n",
    "                q_np[s, a] = exp_r_v\n",
    "        V_next = q_np.max(axis=1)\n",
    "        tol_curr = np.abs(V_next - V).max()\n",
    "        del V\n",
    "        V = np.copy(V_next)\n",
    "        iter_counter += 1\n",
    "    policy = q_np.argmax(axis=1)\n",
    "    ############################\n",
    "    return V, policy\n",
    "\n",
    "def example(env):\n",
    "    \"\"\"Show an example of gym\n",
    "    Parameters\n",
    "        ----------\n",
    "        env: gym.core.Environment\n",
    "            Environment to play on. Must have nS, nA, and P as\n",
    "            attributes.\n",
    "    \"\"\"\n",
    "    env.seed(0);\n",
    "    from gym.spaces import prng; prng.seed(10) # for print the location\n",
    "    # Generate the episode\n",
    "    ob = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        a = env.action_space.sample()\n",
    "        ob, rew, done, _ = env.step(a)\n",
    "        if done:\n",
    "            break\n",
    "    assert done\n",
    "    env.render();\n",
    "\n",
    "def render_single(env, policy):\n",
    "    \"\"\"Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: gym.core.Environment\n",
    "            Environment to play on. Must have nS, nA, and P as\n",
    "            attributes.\n",
    "        Policy: np.array of shape [env.nS]\n",
    "            The action to take at a given state\n",
    "    \"\"\"\n",
    "\n",
    "    episode_reward = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        time.sleep(0.5) # Seconds between frames. Modify as you wish.\n",
    "        a = policy[ob]\n",
    "        ob, rew, done, _ = env.step(a)\n",
    "        episode_reward += rew\n",
    "        if done:\n",
    "            break\n",
    "    assert done\n",
    "    env.render();\n",
    "    print \"Episode reward: %f\" % episode_reward\n",
    "\n",
    "\n",
    "# Feel free to run your own debug code in main!\n",
    "# Play around with these hyperparameters.\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "    print env.__doc__\n",
    "    print \"Here is an example of state, action, reward, and next state\"\n",
    "    example(env)\n",
    "    P_test = env.P\n",
    "    V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, max_iteration=20, tol=1e-3)\n",
    "    V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, max_iteration=20, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 0, 0.0, False)],\n",
       " 1: [(1.0, 4, 0.0, False)],\n",
       " 2: [(1.0, 1, 0.0, False)],\n",
       " 3: [(1.0, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init value function and policy\n",
    "# loop:\n",
    "#    evluate value function given policy\n",
    "#    improve policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter_custom(P, nS, nA, policy, gamma=0.9, max_iteration=1000, tol=1e-3):\n",
    "    # initialize value function\n",
    "    V = np.zeros(nS)\n",
    "    V_prev = np.copy(V)\n",
    "    iter_counter = 0\n",
    "    tol_curr = np.inf\n",
    "    # P[state][action] is tuples\n",
    "    # with (probability, nextstate, reward, terminal)\n",
    "    # loop over Bellman updates:\n",
    "    while iter_counter <= max_iteration or tol_curr >= tol:\n",
    "        for s in range(nS):\n",
    "            q = []\n",
    "            for a in P[s].keys():\n",
    "                p_sp_r_t_lst = P[s][a]\n",
    "                exp_r_v = 0\n",
    "                for p_sp_r_t in p_sp_r_t_lst:\n",
    "                    p, sp, r, t = p_sp_r_t\n",
    "                    exp_r_v += p * (r + gamma * V_prev[sp])\n",
    "                q.append((a, exp_r_v))\n",
    "            # argmax wrt a\n",
    "            V[s] = max(q, key=lambda x: x[1])[0]\n",
    "        tol_curr = np.abs((V - V_prev).max())\n",
    "        iter_counter += 1\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid(array_in):\n",
    "    np.set_printoptions(precision=3)\n",
    "    \n",
    "    print('-' * 20)\n",
    "    for row in range(4):\n",
    "        row_lst = []\n",
    "        for col in range(4):\n",
    "            row_lst.append(round(array_in[row * 4 + col], 3))\n",
    "        print(row_lst)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_deb(P, nS, nA, policy, gamma=0.9, max_iteration=1000, tol=1e-3):    \n",
    "    # initialize value function\n",
    "    V = np.zeros(nS)\n",
    "    V_prev = np.copy(V)\n",
    "    iter_counter = 0\n",
    "    tol_curr = np.inf\n",
    "    # P[state][action] is tuples\n",
    "    # with (probability, nextstate, reward, terminal)\n",
    "    # loop over Bellman updates:\n",
    "    while iter_counter <= max_iteration and tol_curr >= tol:\n",
    "        print(\"iteration: \" + str(iter_counter))\n",
    "        for s in range(nS):\n",
    "            p_sp_r_t_lst = P[s][policy[s]]\n",
    "            exp_r_v = 0.\n",
    "            for p_sp_r_t in p_sp_r_t_lst:\n",
    "                p, sp, r, t = p_sp_r_t\n",
    "#                 print(p_sp_r_t)\n",
    "#                 print(V_prev[sp])\n",
    "                exp_r_v += p * (r + gamma * V_prev[sp])\n",
    "#                 print(exp_r_v)\n",
    "            # argmax wrt a\n",
    "            if s == 10:\n",
    "                print(\"action: \" + str(policy[s]))\n",
    "                print(\"print s': \" + str(sp))\n",
    "                print(\"V(s'): \" + str(V_prev[sp]))\n",
    "            V[s] = exp_r_v\n",
    "        print(V_prev.reshape((4, 4)))\n",
    "        print(V.reshape((4, 4)))\n",
    "        tol_curr = np.abs((V - V_prev).max())\n",
    "        V_prev = np.copy(V)\n",
    "        iter_counter += 1\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement_deb(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"Given the value function from policy improve the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P: dictionary\n",
    "        It is from gym.core.Environment\n",
    "        P[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "    nS: int\n",
    "        number of states\n",
    "    nA: int\n",
    "        number of actions\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new policy: np.ndarray\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "    ############################\n",
    "    V = value_from_policy\n",
    "    q_np = np.zeros((nS, nA), dtype='float')\n",
    "    policy_next = np.zeros(nS, dtype='int')\n",
    "    \n",
    "    for s in range(nS):\n",
    "        # argmax Q(s, a)\n",
    "        # Q(s, a) = E[R(s, a) + gamma * V(s')]\n",
    "        for a in P[s].keys():\n",
    "            p_sp_r_t_lst = P[s][a]\n",
    "            exp_r_v = 0.\n",
    "            for p_sp_r_t in p_sp_r_t_lst:\n",
    "                p, sp, r, t = p_sp_r_t\n",
    "                exp_r_v += p * (r + gamma * V[sp])\n",
    "            q_np[s, a] = exp_r_v\n",
    "    policy_next = q_np.argmax(axis=1) \n",
    "    ############################\n",
    "    return policy_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, max_iteration=20, tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should use the policy_evaluation and policy_improvement methods to\n",
    "    implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P: dictionary\n",
    "        It is from gym.core.Environment\n",
    "        P[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "    nS: int\n",
    "        number of states\n",
    "    nA: int\n",
    "        number of actions\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "    max_iteration: int\n",
    "        The maximum number of iterations to run before stopping. Feel free to change it.\n",
    "    tol: float\n",
    "        Determines when value function has converged.\n",
    "    Returns:\n",
    "    ----------\n",
    "    value function: np.ndarray\n",
    "    policy: np.ndarray\n",
    "    \"\"\"\n",
    "    V = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    ############################\n",
    "    iter_counter = 0\n",
    "    policy_change = np.inf\n",
    "    while iter_counter == 0 or policy_change > 0:\n",
    "        print(\"iteration :\" + str(iter_counter))\n",
    "        print(\"policy_change: \" + str(policy_change))\n",
    "        value_from_policy = policy_evaluation(P, nS, nA, policy, gamma=0.9, max_iteration=max_iteration, tol=tol)\n",
    "        policy_next = policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9)\n",
    "        policy_change = np.abs((policy_next - policy).max())\n",
    "        iter_counter += 1\n",
    "        del policy\n",
    "        policy = np.copy(policy_next)\n",
    "    V = policy_evaluation(P, nS, nA, policy_new, gamma=0.9, max_iteration=max_iteration, tol=tol)\n",
    "    ############################\n",
    "    return V, policy_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.9, max_iteration=20, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    P: dictionary\n",
    "        It is from gym.core.Environment\n",
    "        P[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "    nS: int\n",
    "        number of states\n",
    "    nA: int\n",
    "        number of actions\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "    max_iteration: int\n",
    "        The maximum number of iterations to run before stopping. Feel free to change it.\n",
    "    tol: float\n",
    "        Determines when value function has converged.\n",
    "    Returns:\n",
    "    ----------\n",
    "    value function: np.ndarray\n",
    "    policy: np.ndarray\n",
    "    \"\"\"\n",
    "    ############################\n",
    "    V = np.zeros(nS)\n",
    "    V_next = np.zeros(nS, dtype=int)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    q_np = np.zeros((nS, nA), dtype='float')\n",
    "    iter_counter = 0\n",
    "    tol_curr = np.inf\n",
    "    while iter_counter <= max_iteration and tol_curr >= tol:\n",
    "        for s in range(nS):\n",
    "            for a in range(nA):                \n",
    "                p_sp_r_t_lst = P[s][a]\n",
    "                exp_r_v = 0.\n",
    "                for p_sp_r_t in p_sp_r_t_lst:\n",
    "                    p, sp, r, t = p_sp_r_t\n",
    "                    exp_r_v += p * (r + gamma * V[sp])\n",
    "                q_np[s, a] = exp_r_v\n",
    "        V_next = q_np.max(axis=1)\n",
    "        tol_curr = np.abs(V_next - V).max()\n",
    "        del V\n",
    "        V = np.copy(V_next)\n",
    "        iter_counter += 1\n",
    "    policy = q_np.argmax(axis=1)       \n",
    "    ############################\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59  0.656 0.729 0.656]\n",
      " [0.656 0.    0.81  0.   ]\n",
      " [0.729 0.81  0.9   0.   ]\n",
      " [0.    0.9   1.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "P = env.P\n",
    "nS = env.nS\n",
    "nA = env.nA\n",
    "V = np.zeros(nS)\n",
    "V, policy = value_iteration(P, nS, nA, gamma=0.9, max_iteration=20, tol=1e-3)\n",
    "print(V.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([0, 2]).max() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [4. 3.]\n",
      " [2. 5.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.asarray([\n",
    "    [1, 0],\n",
    "    [4, 3],\n",
    "    [2, 5]\n",
    "], dtype='float')\n",
    "print(q)\n",
    "q.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 3],\n",
       "       [0, 0, 3, 3],\n",
       "       [3, 3, 2, 1],\n",
       "       [1, 1, 1, 0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, high=nA, size=(4, 4), dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(0)\n",
    "env.render()\n",
    "env.reset()\n",
    "policy = np.asarray([\n",
    "    1, 2, 1, 0,\n",
    "    1, 0, 1, 0,\n",
    "    2, 1, 2, 1,\n",
    "    2, 2, 2, 0\n",
    "])\n",
    "\n",
    "policy_iteration(P, nS, nA, gamma=0.9, max_iteration=20, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59 , 0.656, 0.729, 0.656],\n",
       "       [0.656, 0.   , 0.81 , 0.   ],\n",
       "       [0.729, 0.81 , 0.9  , 0.   ],\n",
       "       [0.   , 0.9  , 1.   , 0.   ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.reshape((4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 4, 0.0, False)],\n",
       "  2: [(1.0, 1, 0.0, False)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 1: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 5, 0.0, True)],\n",
       "  2: [(1.0, 2, 0.0, False)],\n",
       "  3: [(1.0, 1, 0.0, False)]},\n",
       " 2: {0: [(1.0, 1, 0.0, False)],\n",
       "  1: [(1.0, 6, 0.0, False)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 3: {0: [(1.0, 2, 0.0, False)],\n",
       "  1: [(1.0, 7, 0.0, True)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 3, 0.0, False)]},\n",
       " 4: {0: [(1.0, 4, 0.0, False)],\n",
       "  1: [(1.0, 8, 0.0, False)],\n",
       "  2: [(1.0, 5, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(1.0, 5, 0.0, True)],\n",
       "  1: [(1.0, 10, 0.0, False)],\n",
       "  2: [(1.0, 7, 0.0, True)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 12, 0.0, True)],\n",
       "  2: [(1.0, 9, 0.0, False)],\n",
       "  3: [(1.0, 4, 0.0, False)]},\n",
       " 9: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 10, 0.0, False)],\n",
       "  3: [(1.0, 5, 0.0, True)]},\n",
       " 10: {0: [(1.0, 9, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 11, 0.0, True)],\n",
       "  3: [(1.0, 6, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(1.0, 12, 0.0, True)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 14, 0.0, False)],\n",
       "  3: [(1.0, 9, 0.0, False)]},\n",
       " 14: {0: [(1.0, 13, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 15, 1.0, True)],\n",
       "  3: [(1.0, 10, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
